{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cbt (C:/Users/34941/.cache/huggingface/datasets/cbt/CN/1.1.0/dc4451a8a4b50cebb78fdb19fa9f964b27fcdcef915467b8b7055a3a8d8cef7b)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb7fb6583c704690be1e3854dfd046c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cbt_dataset = load_dataset('cbt','CN')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_set = cbt_dataset['train']\n",
    "val_set = cbt_dataset['validation']\n",
    "test_set = cbt_dataset['test']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sentences', 'question', 'answer', 'options'],\n    num_rows: 120769\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "{'sentences': ['With almost everything else to make them happy , they wanted one thing : they had no children .',\n  'This vexed the king even more than the queen , who was very clever and learned , and who had hated dolls when she was a child .',\n  'However , she , too in spite of all the books she read and all the pictures she painted , would have been glad enough to be the mother of a little prince .',\n  'The king was anxious to consult the fairies , but the queen would not hear of such a thing .',\n  'She did not believe in fairies : she said that they had never existed ; and that she maintained , though The History of the Royal Family was full of chapters about nothing else .',\n  'Well , at long and at last they had a little boy , who was generally regarded as the finest baby that had ever been seen .',\n  'Even her majesty herself remarked that , though she could never believe all the courtiers told her , yet he certainly was a fine child -- a very fine child .',\n  'Now , the time drew near for the christening party , and the king and queen were sitting at breakfast in their summer parlour talking over it .',\n  'It was a splendid room , hung with portraits of the royal ancestors .',\n  'There was Cinderella , the grandmother of the reigning monarch , with her little foot in her glass slipper thrust out before her .',\n  'There was the Marquis de Carabas , who , as everyone knows , was raised to the throne as prince consort after his marriage with the daughter of the king of the period .',\n  'On the arm of the throne was seated his celebrated cat , wearing boots .',\n  'There , too , was a portrait of a beautiful lady , sound asleep : this was Madame La Belle au Bois-dormant , also an ancestress of the royal family .',\n  'Many other pictures of celebrated persons were hanging on the walls .',\n  \"`` You have asked all the right people , my dear ? ''\",\n  'said the king .',\n  \"`` Everyone who should be asked , '' answered the queen .\",\n  \"`` People are so touchy on these occasions , '' said his majesty .\",\n  \"`` You have not forgotten any of our aunts ? ''\",\n  \"`` No ; the old cats ! ''\"],\n 'question': \"replied the XXXXX ; for the king 's aunts were old-fashioned , and did not approve of her , and she knew it .\",\n 'answer': 'queen',\n 'options': ['ancestors',\n  'baby',\n  'boy',\n  'everyone',\n  'fairies',\n  'mother',\n  'portrait',\n  'queen',\n  'time',\n  'walls']}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [17816, 3152, 2048, 2279, 2073, 284, 787, 606, 3772, 837, 484, 2227, 530, 1517, 1058, 484, 550, 645, 1751, 764, 3256, 705, 1212, 41548, 276, 262, 5822, 772, 517, 621, 262, 16599, 837, 508, 373, 845, 14169, 290, 4499, 837, 290, 508, 550, 16563, 36062, 618, 673, 373, 257, 1200, 764, 3256, 705, 4864, 837, 673, 837, 1165, 287, 15275, 286, 477, 262, 3835, 673, 1100, 290, 477, 262, 5986, 673, 13055, 837, 561, 423, 587, 9675, 1576, 284, 307, 262, 2802, 286, 257, 1310, 19716, 764, 3256, 705, 464, 5822, 373, 18116, 284, 5725, 262, 3148, 444, 837, 475, 262, 16599, 561, 407, 3285, 286, 884, 257, 1517, 764, 3256, 705, 3347, 750, 407, 1975, 287, 3148, 444, 1058, 673, 531, 326, 484, 550, 1239, 11196, 2162, 290, 326, 673, 9456, 837, 996, 383, 7443, 286, 262, 8111, 7884, 373, 1336, 286, 15754, 546, 2147, 2073, 764, 3256, 705, 5779, 837, 379, 890, 290, 379, 938, 484, 550, 257, 1310, 2933, 837, 508, 373, 4143, 11987, 355, 262, 18822, 5156, 326, 550, 1683, 587, 1775, 764, 3256, 705, 6104, 607, 48565, 5223, 24998, 326, 837, 996, 673, 714, 1239, 1975, 477, 262, 2184, 3183, 1297, 607, 837, 1865, 339, 3729, 373, 257, 3734, 1200, 1377, 257, 845, 3734, 1200, 764, 3256, 705, 3844, 837, 262, 640, 9859, 1474, 329, 262, 33826, 3101, 2151, 837, 290, 262, 5822, 290, 16599, 547, 5586, 379, 12607, 287, 511, 3931, 1582, 75, 454, 3375, 625, 340, 764, 3256, 705, 1026, 373, 257, 37196, 2119, 837, 9174, 351, 31725, 286, 262, 15100, 18668, 764, 3256, 705, 1858, 373, 40933, 837, 262, 18410, 286, 262, 40098, 26464, 837, 351, 607, 1310, 2366, 287, 607, 5405, 1017, 14710, 14613, 503, 878, 607, 764, 3256, 705, 1858, 373, 262, 28382, 271, 390, 1879, 397, 292, 837, 508, 837, 355, 2506, 4206, 837, 373, 4376, 284, 262, 19262, 355, 19716, 762, 419, 706, 465, 4845, 351, 262, 4957, 286, 262, 5822, 286, 262, 2278, 764, 3256, 705, 2202, 262, 3211, 286, 262, 19262, 373, 21639, 465, 13943, 3797, 837, 5762, 14412, 764, 3256, 705, 1858, 837, 1165, 837, 373, 257, 18560, 286, 257, 4950, 10846, 837, 2128, 16039, 1058, 428, 373, 43116, 4689, 31974, 35851, 3248, 271, 12, 67, 579, 415, 837, 635, 281, 10790, 601, 286, 262, 15100, 1641, 764, 3256, 705, 7085, 584, 5986, 286, 13943, 6506, 547, 10938, 319, 262, 7714, 764, 3256, 366, 15506, 921, 423, 1965, 477, 262, 826, 661, 837, 616, 13674, 5633, 10148, 1600, 705, 30079, 262, 5822, 764, 3256, 366, 15506, 11075, 508, 815, 307, 1965, 837, 10148, 9373, 262, 16599, 22135, 11, 366, 15506, 4380, 389, 523, 3638, 88, 319, 777, 12432, 837, 10148, 531, 465, 48565, 22135, 11, 366, 15506, 921, 423, 407, 11564, 597, 286, 674, 257, 34115, 5633, 10148, 1600, 366, 15506, 1400, 2162, 262, 1468, 11875, 5145, 10148, 8973, 8712, 262, 16599, 2162, 329, 262, 1395, 24376, 705, 82, 257, 34115, 547, 1468, 12, 28776, 837, 290, 750, 407, 14762, 286, 607, 837, 290, 673, 2993, 340, 764, 257, 34115, 5156, 5822, 26464, 2802, 12432, 19716, 15275, 19262, 640], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_set[1]\n",
    "context, question, answer, options = example['sentences'], example['question'], example['answer'], example['options']\n",
    "input_text = f\"{context} {question} {' '.join(options)}\"\n",
    "tokenizer(input_text, add_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "task: write a preprocess function s.t. for an input: context, question, answer, options, output a format that we can put into data loader:\n",
    "做成input_id, labels, 然后把input_id和labels都padding成一样长就行了应该。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2079/120769 [00:04<04:18, 458.96it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 120769/120769 [04:44<00:00, 425.19it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set_lst = []\n",
    "label_lst = []\n",
    "attn_lst = []\n",
    "padding = -100\n",
    "max_length = 1024\n",
    "for i in tqdm(range(len(train_set))):\n",
    "    example = train_set[i]\n",
    "    context, question, answer, options = example['sentences'], example['question'], example['answer'], example['options']\n",
    "    input_text = f\"{context} {question} {' '.join(options)}\"\n",
    "    input_ids,attention_mask = tokenizer(input_text, add_special_tokens=True)\n",
    "    labels = [0]*len(input_ids)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    labels[-len(options) + options.index(answer)] = answer_id\n",
    "    if len(input_ids)<=1024:\n",
    "        padding_length = max_length-len(input_ids)\n",
    "        input_ids+=[padding]*padding_length\n",
    "        labels+=[padding]*padding_length\n",
    "        attention_mask+=[padding]*padding_length\n",
    "        train_set_lst.append(torch.tensor(input_ids))\n",
    "        label_lst.append(torch.tensor(labels))\n",
    "        attn_lst.append(attention_mask)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "dataset = torch.utils.data.TensorDataset(*train_set_lst,*attn_lst,*label_lst)\n",
    "train_dataloader = DataLoader(dataset,batch_size=8,shuffle=True)\n",
    "print('finish')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "1024"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set_lst[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [42]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m input_tensors \u001B[38;5;241m=\u001B[39m batch[:\u001B[38;5;241m1024\u001B[39m]\n\u001B[0;32m      3\u001B[0m target_tensors \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;241m1024\u001B[39m:]\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43minput_tensors\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36m<cell line: 31>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     29\u001B[0m num_warmup_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(num_training_steps \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m     30\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n\u001B[1;32m---> 31\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_dataloader, optimizer, scheduler, num_epochs)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(num_epochs)):\n\u001B[0;32m      9\u001B[0m     total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 10\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m input_ids, labels \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[0;32m     12\u001B[0m         input_ids, labels \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     13\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m model(input_ids, labels\u001B[38;5;241m=\u001B[39mlabels)\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define model and optimizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define training loop\n",
    "def train(model, train_dataloader, optimizer, scheduler=None, num_epochs=1):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_loss = 0\n",
    "        #就这个下面一行我不知道要怎么把train_dataloader里面的东西读出来。。应该是要input_ids, attention_mask, 和labels,三个tensor\n",
    "        for input_ids, labels in train_dataloader:\n",
    "\n",
    "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_dataloader)}\")\n",
    "\n",
    "# Load dataset and create dataloader\n",
    "\n",
    "\n",
    "# Fine-tune model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "num_training_steps = len(train_dataloader) * 3\n",
    "num_warmup_steps = int(num_training_steps * 0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "train(model, train_dataloader, optimizer, scheduler=scheduler, num_epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "72"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}