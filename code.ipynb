{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Model, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cbt (C:/Users/34941/.cache/huggingface/datasets/cbt/CN/1.1.0/dc4451a8a4b50cebb78fdb19fa9f964b27fcdcef915467b8b7055a3a8d8cef7b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646df4053ae84dbba501c4093a4d8d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cbt_dataset = load_dataset('cbt','CN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set = cbt_dataset['train']\n",
    "val_set = cbt_dataset['validation']\n",
    "test_set = cbt_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentences', 'question', 'answer', 'options'],\n",
       "    num_rows: 120769\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [17816, 3152, 2048, 2279, 2073, 284, 787, 606, 3772, 837, 484, 2227, 530, 1517, 1058, 484, 550, 645, 1751, 764, 3256, 705, 1212, 41548, 276, 262, 5822, 772, 517, 621, 262, 16599, 837, 508, 373, 845, 14169, 290, 4499, 837, 290, 508, 550, 16563, 36062, 618, 673, 373, 257, 1200, 764, 3256, 705, 4864, 837, 673, 837, 1165, 287, 15275, 286, 477, 262, 3835, 673, 1100, 290, 477, 262, 5986, 673, 13055, 837, 561, 423, 587, 9675, 1576, 284, 307, 262, 2802, 286, 257, 1310, 19716, 764, 3256, 705, 464, 5822, 373, 18116, 284, 5725, 262, 3148, 444, 837, 475, 262, 16599, 561, 407, 3285, 286, 884, 257, 1517, 764, 3256, 705, 3347, 750, 407, 1975, 287, 3148, 444, 1058, 673, 531, 326, 484, 550, 1239, 11196, 2162, 290, 326, 673, 9456, 837, 996, 383, 7443, 286, 262, 8111, 7884, 373, 1336, 286, 15754, 546, 2147, 2073, 764, 3256, 705, 5779, 837, 379, 890, 290, 379, 938, 484, 550, 257, 1310, 2933, 837, 508, 373, 4143, 11987, 355, 262, 18822, 5156, 326, 550, 1683, 587, 1775, 764, 3256, 705, 6104, 607, 48565, 5223, 24998, 326, 837, 996, 673, 714, 1239, 1975, 477, 262, 2184, 3183, 1297, 607, 837, 1865, 339, 3729, 373, 257, 3734, 1200, 1377, 257, 845, 3734, 1200, 764, 3256, 705, 3844, 837, 262, 640, 9859, 1474, 329, 262, 33826, 3101, 2151, 837, 290, 262, 5822, 290, 16599, 547, 5586, 379, 12607, 287, 511, 3931, 1582, 75, 454, 3375, 625, 340, 764, 3256, 705, 1026, 373, 257, 37196, 2119, 837, 9174, 351, 31725, 286, 262, 15100, 18668, 764, 3256, 705, 1858, 373, 40933, 837, 262, 18410, 286, 262, 40098, 26464, 837, 351, 607, 1310, 2366, 287, 607, 5405, 1017, 14710, 14613, 503, 878, 607, 764, 3256, 705, 1858, 373, 262, 28382, 271, 390, 1879, 397, 292, 837, 508, 837, 355, 2506, 4206, 837, 373, 4376, 284, 262, 19262, 355, 19716, 762, 419, 706, 465, 4845, 351, 262, 4957, 286, 262, 5822, 286, 262, 2278, 764, 3256, 705, 2202, 262, 3211, 286, 262, 19262, 373, 21639, 465, 13943, 3797, 837, 5762, 14412, 764, 3256, 705, 1858, 837, 1165, 837, 373, 257, 18560, 286, 257, 4950, 10846, 837, 2128, 16039, 1058, 428, 373, 43116, 4689, 31974, 35851, 3248, 271, 12, 67, 579, 415, 837, 635, 281, 10790, 601, 286, 262, 15100, 1641, 764, 3256, 705, 7085, 584, 5986, 286, 13943, 6506, 547, 10938, 319, 262, 7714, 764, 3256, 366, 15506, 921, 423, 1965, 477, 262, 826, 661, 837, 616, 13674, 5633, 10148, 1600, 705, 30079, 262, 5822, 764, 3256, 366, 15506, 11075, 508, 815, 307, 1965, 837, 10148, 9373, 262, 16599, 22135, 11, 366, 15506, 4380, 389, 523, 3638, 88, 319, 777, 12432, 837, 10148, 531, 465, 48565, 22135, 11, 366, 15506, 921, 423, 407, 11564, 597, 286, 674, 257, 34115, 5633, 10148, 1600, 366, 15506, 1400, 2162, 262, 1468, 11875, 5145, 10148, 8973, 8712, 262, 16599, 2162, 329, 262, 1395, 24376, 705, 82, 257, 34115, 547, 1468, 12, 28776, 837, 290, 750, 407, 14762, 286, 607, 837, 290, 673, 2993, 340, 764, 257, 34115, 5156, 5822, 26464, 2802, 12432, 19716, 15275, 19262, 640], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_set[1]\n",
    "context, question, answer, options = example['sentences'], example['question'], example['answer'], example['options']\n",
    "input_text = f\"{context} {question} {' '.join(options)}\"\n",
    "tokenizer(input_text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_ids,attention_mask = tokenizer(input_text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_ids'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "task: write a preprocess function s.t. for an input: context, question, answer, options, output a format that we can put into data loader:\n",
    "做成input_id, labels, 然后把input_id和labels都padding成一样长就行了应该。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████████████████████████████████████████▎          | 2080/2415 [00:08<00:01, 263.60it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2415/2415 [00:10<00:00, 229.90it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set_lst = []\n",
    "label_lst = []\n",
    "attn_lst = []\n",
    "token_type_id_lst = []\n",
    "padding = 50256\n",
    "max_length = 1024\n",
    "for i in tqdm(range(int(len(train_set)/50))):\n",
    "    example = train_set[i]\n",
    "    context, question, answer, options = example['sentences'], example['question'], example['answer'], example['options']\n",
    "    input_text = f\"{context} {question} {' '.join(options)}\"\n",
    "    input_ids,attention_mask = tokenizer(input_text, add_special_tokens=True)['input_ids'],tokenizer(input_text, add_special_tokens=True)['attention_mask']\n",
    "    context_length = len(input_ids)-len(question)-10\n",
    "    question_length = len(question)\n",
    "\n",
    "    option_length = 10  # divide by the number of options\n",
    "    token_type_id = [0] * context_length + [1] * question_length + [2] * option_length\n",
    "    '''\n",
    "    labels = [0]*len(input_ids)\n",
    "    answer_id = tokenizer.encode(answer)[0]\n",
    "    labels[-len(options) + options.index(answer)] = answer_id\n",
    "    '''\n",
    "    label = options.index(answer)\n",
    "    if len(input_ids)<=1024:\n",
    "        padding_length = max_length-len(input_ids)\n",
    "        input_ids+=[padding]*padding_length\n",
    "        #labels+=[padding]*padding_length\n",
    "        attention_mask+=[padding]*padding_length\n",
    "        token_type_id+=[padding]*padding_length\n",
    "\n",
    "        train_set_lst.append(input_ids)\n",
    "        label_lst.append(label)\n",
    "        attn_lst.append(attention_mask)\n",
    "        token_type_id_lst.append(token_type_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "example = train_set[0]\n",
    "context, question, answer, options = example['sentences'], example['question'], example['answer'], example['options']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(torch.tensor(train_set_lst),torch.tensor(attn_lst),torch.tensor(token_type_id_lst),torch.tensor(label_lst))\n",
    "train_dataloader = DataLoader(dataset,batch_size=2,shuffle=True)\n",
    "print('finish')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class GPT2ForMultipleChoice(nn.Module):\n",
    "    def __init__(self, gpt2_model, num_choices=10):\n",
    "        super().__init__()\n",
    "        self.gpt2 = gpt2_model\n",
    "        self.classifier = nn.Linear(self.gpt2.config.n_embd, num_choices)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        attention_mask = attention_mask.view(-1, input_shape[-1]) if attention_mask is not None else None\n",
    "        token_type_ids = token_type_ids.view(-1, input_shape[-1]) if token_type_ids is not None else None\n",
    "\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        pooled_output = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "        '''\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, input_shape[1]), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits.view(-1, input_shape[1], self.classifier.out_features)\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.pad_token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "a = inputs['input_ids']\n",
    "print(a)\n",
    "outputs = model(a)\n",
    "outputs[0][:,-1,:].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model(torch.tensor(train_set_lst[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor(train_set_lst[0]).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#model2 = GPT2ForMultipleChoice(model,num_choices=10)\n",
    "for batch in train_dataloader:\n",
    "    # Unpack batch\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    input_ids,attention_mask,token_type_ids,labels = batch[0].to(device),batch[1].to(device),batch[2].to(device),batch[3].to(device)\n",
    "    out = model(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "    print(out[0][:,0,:].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Define model and optimizer\n",
    "#model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define training loop\n",
    "def train(model, train_dataloader, optimizer, scheduler=None, num_epochs=1):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_loss = 0\n",
    "        #就这个下面一行我不知道要怎么把train_dataloader里面的东西读出来。。应该是要input_ids, attention_mask, 和labels,三个tensor\n",
    "        for input_ids, attention_mask,token_type_ids,labels in train_dataloader:\n",
    "\n",
    "            input_ids, attention_mask,token_type_ids, labels = input_ids.to(device),attention_mask.to(device), token_type_ids.to(device),labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids = input_ids,attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "            criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "            loss = criterion(logits,labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_dataloader)}\")\n",
    "\n",
    "# Load dataset and create dataloader\n",
    "\n",
    "model2 = GPT2ForMultipleChoice(model,num_choices=10)\n",
    "# Fine-tune model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model2.to(device)\n",
    "num_training_steps = len(train_dataloader) * 3\n",
    "num_warmup_steps = int(num_training_steps * 0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "train(model2, train_dataloader, optimizer, scheduler=scheduler, num_epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!conda list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "一个问题变成10个sample，每个变成 binary classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "hw3",
   "language": "python",
   "display_name": "hw3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}