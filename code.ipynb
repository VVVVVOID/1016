{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\34941\\anaconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\34941\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-win_amd64.whl (20.6 MB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\34941\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\34941\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\34941\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\34941\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: dill, xxhash, responses, pyarrow, multiprocess, huggingface-hub, datasets\n",
      "Successfully installed datasets-2.12.0 dill-0.3.6 huggingface-hub-0.14.1 multiprocess-0.70.14 pyarrow-11.0.0 responses-0.18.0 xxhash-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cbt (C:/Users/34941/.cache/huggingface/datasets/cbt/CN/1.1.0/dc4451a8a4b50cebb78fdb19fa9f964b27fcdcef915467b8b7055a3a8d8cef7b)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e7cd0e0164440d882d8f1a5d561fd74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cbt_dataset = load_dataset('cbt','CN')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_set = cbt_dataset['train']\n",
    "val_set = cbt_dataset['validation']\n",
    "test_set = cbt_dataset['test']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sentences', 'question', 'answer', 'options'],\n    num_rows: 120769\n})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2097a8aa0fc84c64a71339f1e3c77792"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\34941\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\34941\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93a7efc11b4343a093265c7d4653f6c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7c83a6f9501497b8a1d56dee1bb7694"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5d9f17d9970404783aa970e1e7b8383"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cd62f6d59704daa8a11f51dc146f2da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "task: write a preprocess function s.t. for an input: context, question, answer, options, output a format that we can put into data loader:\n",
    "做成input_id, labels, 然后把input_id和labels都padding成一样长就行了应该。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        context, question, answer, options = self.data[index]\n",
    "        input_text = f\"{context} {question} {' '.join(options)}\"\n",
    "        input_ids = self.tokenizer.encode(input_text, add_special_tokens=True)\n",
    "        labels = [0] * len(input_ids)\n",
    "        answer_id = self.tokenizer.encode(answer)[0]\n",
    "        labels[-len(options) + options.index(answer)] = answer_id\n",
    "        return torch.tensor(input_ids), torch.tensor(labels)\n",
    "这个是可以用得上的"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "context, question, answer, options = train_set[0]['sentences'], train_set[0]['question'], train_set[0]['answer'],train_set[0]['options']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "input_text = f\"{context} {question} {' '.join(options)}\"\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "labels = [0] * len(input_ids)\n",
    "answer_id = tokenizer.encode(answer)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "labels[-len(options) + options.index(answer)] = answer_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17816,  3152,  2048,  2279,  2073,   284,   787,   606,  3772,   837,\n",
      "          484,  2227,   530,  1517,  1058,   484,   550,   645,  1751,   764,\n",
      "         3256,   705,  1212, 41548,   276,   262,  5822,   772,   517,   621,\n",
      "          262, 16599,   837,   508,   373,   845, 14169,   290,  4499,   837,\n",
      "          290,   508,   550, 16563, 36062,   618,   673,   373,   257,  1200,\n",
      "          764,  3256,   705,  4864,   837,   673,   837,  1165,   287, 15275,\n",
      "          286,   477,   262,  3835,   673,  1100,   290,   477,   262,  5986,\n",
      "          673, 13055,   837,   561,   423,   587,  9675,  1576,   284,   307,\n",
      "          262,  2802,   286,   257,  1310, 19716,   764,  3256,   705,   464,\n",
      "         5822,   373, 18116,   284,  5725,   262,  3148,   444,   837,   475,\n",
      "          262, 16599,   561,   407,  3285,   286,   884,   257,  1517,   764,\n",
      "         3256,   705,  3347,   750,   407,  1975,   287,  3148,   444,  1058,\n",
      "          673,   531,   326,   484,   550,  1239, 11196,  2162,   290,   326,\n",
      "          673,  9456,   837,   996,   383,  7443,   286,   262,  8111,  7884,\n",
      "          373,  1336,   286, 15754,   546,  2147,  2073,   764,  3256,   705,\n",
      "         5779,   837,   379,   890,   290,   379,   938,   484,   550,   257,\n",
      "         1310,  2933,   837,   508,   373,  4143, 11987,   355,   262, 18822,\n",
      "         5156,   326,   550,  1683,   587,  1775,   764,  3256,   705,  6104,\n",
      "          607, 48565,  5223, 24998,   326,   837,   996,   673,   714,  1239,\n",
      "         1975,   477,   262,  2184,  3183,  1297,   607,   837,  1865,   339,\n",
      "         3729,   373,   257,  3734,  1200,  1377,   257,   845,  3734,  1200,\n",
      "          764,  3256,   705,  3844,   837,   262,   640,  9859,  1474,   329,\n",
      "          262, 33826,  3101,  2151,   837,   290,   262,  5822,   290, 16599,\n",
      "          547,  5586,   379, 12607,   287,   511,  3931,  1582,    75,   454,\n",
      "         3375,   625,   340,   764,  3256,   705,  1026,   373,   257, 37196,\n",
      "         2119,   837,  9174,   351, 31725,   286,   262, 15100, 18668,   764,\n",
      "         3256,   705,  1858,   373, 40933,   837,   262, 18410,   286,   262,\n",
      "        40098, 26464,   837,   351,   607,  1310,  2366,   287,   607,  5405,\n",
      "         1017, 14710, 14613,   503,   878,   607,   764,  3256,   705,  1858,\n",
      "          373,   262, 28382,   271,   390,  1879,   397,   292,   837,   508,\n",
      "          837,   355,  2506,  4206,   837,   373,  4376,   284,   262, 19262,\n",
      "          355, 19716,   762,   419,   706,   465,  4845,   351,   262,  4957,\n",
      "          286,   262,  5822,   286,   262,  2278,   764,  3256,   705,  2202,\n",
      "          262,  3211,   286,   262, 19262,   373, 21639,   465, 13943,  3797,\n",
      "          837,  5762, 14412,   764,  3256,   705,  1858,   837,  1165,   837,\n",
      "          373,   257, 18560,   286,   257,  4950, 10846,   837,  2128, 16039,\n",
      "         1058,   428,   373, 43116,  4689, 31974, 35851,  3248,   271,    12,\n",
      "           67,   579,   415,   837,   635,   281, 10790,   601,   286,   262,\n",
      "        15100,  1641,   764,  3256,   705,  7085,   584,  5986,   286, 13943,\n",
      "         6506,   547, 10938,   319,   262,  7714,   764,  3256,   366, 15506,\n",
      "          921,   423,  1965,   477,   262,   826,   661,   837,   616, 13674,\n",
      "         5633, 10148,  1600,   705, 30079,   262,  5822,   764,  3256,   366,\n",
      "        15506, 11075,   508,   815,   307,  1965,   837, 10148,  9373,   262,\n",
      "        16599, 22135,    11,   366, 15506,  4380,   389,   523,  3638,    88,\n",
      "          319,   777, 12432,   837, 10148,   531,   465, 48565, 22135,    11,\n",
      "          366, 15506,   921,   423,   407, 11564,   597,   286,   674,   257,\n",
      "        34115,  5633, 10148,  1600,   366, 15506,  1400,  2162,   262,  1468,\n",
      "        11875,  5145, 10148,  8973,  8712,   262,  1395, 24376,  2162,   329,\n",
      "          262,  5822,   705,    82,   257, 34115,   547,  1468,    12, 28776,\n",
      "          837,   290,   750,   407, 14762,   286,   607,   837,   290,   673,\n",
      "         2993,   340,   764, 18668,  5156,  2933,  2506,  3148,   444,  2802,\n",
      "        18560, 16599,   640,  7714]) tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0, 4188,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(input_ids), torch.tensor(labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "'queen'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]['answer']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class ReadingComprehensionDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = []\n",
    "        for example in dataset:\n",
    "            context = example['sentences']\n",
    "            question = example['question']\n",
    "            options = example['options']\n",
    "            answer = example['answer']\n",
    "            self.data.append((context, question, answer, options))\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def getitem(self, index):\n",
    "        context, question, answer, options = self.data[index]\n",
    "        input_text = f\"{context} {question} {' '.join(options)}\"\n",
    "        input_ids = self.tokenizer.encode(input_text, add_special_tokens=True)\n",
    "        labels = [0] * len(input_ids)\n",
    "        answer_id = self.tokenizer.encode(answer)[0]\n",
    "        labels[-len(options) + options.index(answer)] = answer_id\n",
    "        return torch.tensor(input_ids), torch.tensor(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "dataset = ReadingComprehensionDataset(train_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def get_data_loader(dataset, batch_size, shuffle=True):\n",
    "    def collate_fn(batch):\n",
    "        input_ids = [torch.tensor(example['input_ids'], dtype=torch.long) for example in batch]\n",
    "        labels = [torch.tensor(example['label'], dtype=torch.long) for example in batch]\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100) # ignore loss for padding tokens\n",
    "        return input_ids, labels\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "    return dataloader\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "dataset = ReadingComprehensionDataset(train_set)\n",
    "train_dataloader = get_data_loader(dataset, batch_size=8, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\34941\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [42]\u001B[0m, in \u001B[0;36m<cell line: 35>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     33\u001B[0m num_warmup_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(num_training_steps \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m     34\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n\u001B[1;32m---> 35\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [42]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_dataloader, optimizer, scheduler, num_epochs)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m     11\u001B[0m     total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 12\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m input_ids, labels \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[0;32m     13\u001B[0m         \u001B[38;5;28mprint\u001B[39m(input_ids)\n\u001B[0;32m     14\u001B[0m         \u001B[38;5;28mprint\u001B[39m(labels)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [39]\u001B[0m, in \u001B[0;36mget_data_loader.<locals>.collate_fn\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcollate_fn\u001B[39m(batch):\n\u001B[1;32m----> 6\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mtensor(example[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong) \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m batch]\n\u001B[0;32m      7\u001B[0m     labels \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mtensor(example[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong) \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m batch]\n\u001B[0;32m      8\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m pad_sequence(input_ids, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, padding_value\u001B[38;5;241m=\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mpad_token_id)\n",
      "Input \u001B[1;32mIn [39]\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcollate_fn\u001B[39m(batch):\n\u001B[1;32m----> 6\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[43mexample\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong) \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m batch]\n\u001B[0;32m      7\u001B[0m     labels \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mtensor(example[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong) \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m batch]\n\u001B[0;32m      8\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m pad_sequence(input_ids, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, padding_value\u001B[38;5;241m=\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mpad_token_id)\n",
      "\u001B[1;31mTypeError\u001B[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Define model and optimizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define training loop\n",
    "def train(model, train_dataloader, optimizer, scheduler=None, num_epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for input_ids, labels in train_dataloader:\n",
    "            print(input_ids)\n",
    "            print(labels)\n",
    "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_dataloader)}\")\n",
    "\n",
    "# Load dataset and create dataloader\n",
    "\n",
    "\n",
    "\n",
    "# Fine-tune model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "num_training_steps = len(train_dataloader) * 3\n",
    "num_warmup_steps = int(num_training_steps * 0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "train(model, train_dataloader, optimizer, scheduler=scheduler, num_epochs=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}